# Automation, parallelisation and the modern biologist
##### 2025-09-24

## Scale demands automation/parallel processes
As someone who started their scientific career in the wet lab dealing with single strains and cloning, I was never taught to think at scale. Only when I started dealing with large datasets in the computational space, did this become an essential skill and design philosophy that I needed to develop. This sort of thinking has not only improved my capabilities and speed at which I can figure things out, but it has also improved my laboratory planning and execution. Nowadays, I won't step foot in the lab until I've done the prerequisite bioinformatics checks. This means the lab becomes a place of hypothesis validation/rejection rather than meandering discovery. This bioinformatically-driven decision-making covers everything from strain selection, power calculations, *in silico* PCR, antibiotic/antiseptic choice - all in an effort to maximise experimental discovery while minimising time spent (and plastic waste generated). Though I am a naturally lazy person, so maybe that's the real reason and everything else is *post hoc* rationalisation.

The more advanced my computational skills have become, the more frustrated I get when tasks cannot be automated or parallelised. If I have to open Excel or navigate a GUI, each additional click feels like a personal attack on my limited time, which will often require repeating the same task multiple times.


For example, I see single-channel pipetting as a `for` loop:
```bash
for i in {1..8}; do
    mv "tube_${i}/cells" "new_tube_${i}/media+cells"
done
```
For loops are great because they are automated, but it still suffers from sequentiality, making it slow. Single-channel pipettes have the worst of both worlds in that they are both manual and sequential. Nowadays in the lab (when I get to go in), I try to design experiments at scale with multi-channel pipettes, keeping `gnu parallel` in mind:
```bash
ls -1 tube_*/cells | parallel -j 8 '
  dir=$(dirname {})
  base=$(basename {})
  mv {} new_${dir}/$base
'
```
This means I can test more bacterial isolates, more substrates/reagents and ensure statistical power is reached with replicates. It feels more complex to set up, but the payoff is worth it. And yes I am aware one shouldn't multi-thread I/O-bound tasks like `mv`. But the metaphor works for pipettes. But more generally, this serves as a self-reminder that I should spend more time planning and less time executing.


## Bioinformatics is part of the modern biologists' toolkit

Look at any of the top modern, biology papers, they have some, if not large computational/bioinformatic components. This includes structural, sequence analysis, population biology etc. The simple fact is computational approaches are high-yield science.

Yet why is bioinformatics and data science missing from many biology/biomedical science degrees? These should be 100% core units. We actually have difficulty recruiting students because their experience with undergraduate 
error is with universities and not integrating bioinformatics

For many wet lab researchers including those in training, may feel overwhelmed or intimidated by comptuational/bioinformatic methods. 

as they don't know how to code. The great part about 2025 bioinformatics is you don't need to. Most things you need are plug-and-play. You just need 

no_spaces_in_names

## Setting up the blog with as much command-line as possible
set up this blog with VSCode, Github...screenshots

@hattoriCorrelationInterferonsAutoimmune2025
@hoheiselAutoantibodiesArgininerichSequences2024

# References


